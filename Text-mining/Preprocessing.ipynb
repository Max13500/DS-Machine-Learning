{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e888419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee8a68",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d17ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Souffrez qu'Amour cette nuit vous réveille.\", 'Par mes soupirs laissez-vous enflammer.', 'Vous dormez trop, adorable merveille.', \"Car c'est dormir que de ne point aimer.\"]\n"
     ]
    }
   ],
   "source": [
    "txt = \"Souffrez qu'Amour cette nuit vous réveille. Par mes soupirs laissez-vous enflammer. Vous dormez trop, adorable merveille. Car c'est dormir que de ne point aimer.\"\n",
    "\n",
    "# Tokenization d'un paragraphe en phrases\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "print(tokenizer.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714c6902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Souffrez', \"qu'Amour\", 'cette', 'nuit', 'vous', 'réveille', '.', 'Par', 'mes', 'soupirs', 'laissez-vous', 'enflammer', '.', 'Vous', 'dormez', 'trop', ',', 'adorable', 'merveille', '.', 'Car', \"c'est\", 'dormir', 'que', 'de', 'ne', 'point', 'aimer', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\maxbe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization d'une phrase en mots\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "mots = word_tokenize(txt, language = 'french')\n",
    "print(mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f970b626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Souffrez', 'Amour', 'cette', 'nuit', 'vous', 'réveille', 'soupirs', 'laissez', 'vous', 'enflammer', 'Vous', 'dormez', 'trop', 'adorable', 'merveille', 'dormir', 'point', 'aimer']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation personnalisée : on ne conserve que les mots de 4 caractères ou plus\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"\\w{4,}\")\n",
    "tokens = tokenizer.tokenize(txt)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81bad67",
   "metadata": {},
   "source": [
    "### Filtrage des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089d92d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eues', 'le', 'êtes', 'serai', 'étiez', 'aviez', 'me', 'avec', 'étaient', 'serait', 'étées', 'sois', 'm', 'j', 'nos', 'serons', 'même', 'l', 'ayants', 'ayons', 'eût', 'aient', 'eusses', 'était', 'ses', 'se', 'son', 'qu', 'ayante', 'dans', 'soyons', 'fût', 'étants', 'fusse', 'fûtes', 'on', 'serions', 'eue', 'au', 'ou', 'sera', 'furent', 'eusse', 'eûmes', 'auront', 'étant', 'ayant', 'aurions', 'étantes', 'ils', 'avais', 'avait', 'fûmes', 'aura', 'est', 'fussions', 'c', 'suis', 'je', 'sont', 'leur', 'par', 'seront', 'aurons', 'fussent', 'ton', 'avaient', 'pas', 'ne', 'il', 'qui', 'fut', 'avions', 'seriez', 'eussent', 'eus', 'nous', 'de', 'étante', 'aurez', 'seras', 'soyez', 'étions', 'aurai', 'ai', 'tes', 'ont', 'soient', 'étée', 'ayez', 'aux', 'seraient', 'n', 'fussiez', 'auras', 'aurais', 'avons', 'étais', 'fus', 'tu', 'sommes', 'moi', 'à', 'eurent', 'mais', 'sur', 'étés', 'ce', 'une', 'votre', 'ces', 'serais', 'eussiez', 'la', 'aurait', 'en', 'et', 'serez', 'notre', 'sa', 'les', 'mon', 'un', 'fusses', 'te', 'es', 'eu', 'eut', 'mes', 'eussions', 'eûtes', 'été', 'vos', 'avez', 'auriez', 'auraient', 's', 'aie', 'vous', 't', 'elle', 'toi', 'd', 'aies', 'soit', 'ma', 'as', 'du', 'ta', 'que', 'lui', 'des', 'eux', 'ayantes', 'y', 'ait', 'pour'}\n"
     ]
    }
   ],
   "source": [
    "# Importer stopwords de la classe nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialiser la liste des stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c745017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Souffrez', \"qu'Amour\", 'cette', 'nuit', 'réveille', 'Par', 'soupirs', 'laissez-vous', 'enflammer', 'Vous', 'dormez', 'trop', 'adorable', 'merveille', 'Car', \"c'est\", 'dormir', 'point', 'aimer']\n"
     ]
    }
   ],
   "source": [
    "# Enrichir la liste des stopwords\n",
    "stop_words.update([\".\",\",\"])\n",
    "\n",
    "# Fonction de retrait des stopwords d'une liste de mots\n",
    "def stop_words_filtering(words):\n",
    "    return ([x for x in words if x not in stop_words])\n",
    "\n",
    "# Suppression des stopwords du texte tokenisé en mots\n",
    "mots_without_stopwords = stop_words_filtering(mots)\n",
    "print(mots_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf71f51",
   "metadata": {},
   "source": [
    "### Vectorisation Bag of Words (après tokenisation et retrait des stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e9f3f",
   "metadata": {},
   "source": [
    "Rmq : La vectorisation intègre une tokenisation, une mise en minuscule et un retrait de ponctuation par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "666586bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire :\n",
      "{'souffrez': 12, 'amour': 2, 'cette': 3, 'nuit': 9, 'vous': 15, 'réveille': 11, 'soupirs': 13, 'laissez': 7, 'enflammer': 6, 'dormez': 4, 'trop': 14, 'adorable': 0, 'merveille': 8, 'dormir': 5, 'point': 10, 'aimer': 1}\n",
      "Word of Bags :\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "from  sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "tokens_vect = vectorizer.fit_transform(tokens)\n",
    "print (\"Vocabulaire :\")\n",
    "print (vectorizer.vocabulary_)\n",
    "print (\"Word of Bags :\")\n",
    "print (tokens_vect.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb604f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Autre exemple\n",
    "print (vectorizer.transform([\"laissez-vous enflammer, demoiselle\",\"dormez vous cette nuit, vous ?\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611698e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire :\n",
      "{'souffrez': 12, 'amour': 2, 'cette': 3, 'nuit': 9, 'vous': 15, 'réveille': 11, 'soupirs': 13, 'laissez': 7, 'enflammer': 6, 'dormez': 4, 'trop': 14, 'adorable': 0, 'merveille': 8, 'dormir': 5, 'point': 10, 'aimer': 1}\n",
      "Word of Bags :\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "tokens_vect = vectorizer_tfidf.fit_transform(tokens)\n",
    "print (\"Vocabulaire :\")\n",
    "print (vectorizer_tfidf.vocabulary_)\n",
    "print (\"Word of Bags :\")\n",
    "print (tokens_vect.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2ce632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.61791199 0.61791199 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48617852]\n",
      " [0.         0.         0.         0.42732422 0.42732422 0.\n",
      "  0.         0.         0.         0.42732422 0.         0.\n",
      "  0.         0.         0.         0.67244482]]\n"
     ]
    }
   ],
   "source": [
    "# Autre exemple\n",
    "print (vectorizer_tfidf.transform([\"laissez-vous enflammer, demoiselle\",\"dormez vous cette nuit, vous ?\"]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845f678",
   "metadata": {},
   "source": [
    "### Racinisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d52af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sérieux\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "print(stemmer.stem('sérieusement'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e38c132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vous', 'merveil', 'dorm', 'soupir', 'enflamm', 'cet', 'nuit', 'trop', 'ador', 'réveil', 'point', 'aim', 'souffr', 'amour', 'laiss']\n"
     ]
    }
   ],
   "source": [
    "def stemming(mots):\n",
    "    stemmer = FrenchStemmer()\n",
    "    mots_racines = [stemmer.stem(mot) for mot in mots]\n",
    "    return (list(set(mots_racines)))\n",
    "\n",
    "print(stemming(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46082e19",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "593998af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxbe\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('meet', 'meeting')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Calculer le lemme du mot meeting\n",
    "wordnet_lemmatizer.lemmatize('meeting', pos='v'), wordnet_lemmatizer.lemmatize('meeting', pos='n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
